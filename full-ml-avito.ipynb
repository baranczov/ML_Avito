{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14593623,"sourceType":"datasetVersion","datasetId":9321923}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:52.242807Z","iopub.execute_input":"2026-01-28T17:20:52.243408Z","iopub.status.idle":"2026-01-28T17:20:52.250453Z","shell.execute_reply.started":"2026-01-28T17:20:52.243349Z","shell.execute_reply":"2026-01-28T17:20:52.249812Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets-avito/train-dset.parquet\n/kaggle/input/datasets-avito/test-dset-small.parquet\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Preprocessing + TF-IDF (Train)","metadata":{}},{"cell_type":"code","source":"import os, gc, json\nimport numpy as np\nimport pandas as pd\n\nimport pyarrow as pa\nimport pyarrow.dataset as ds\n\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:52.251771Z","iopub.execute_input":"2026-01-28T17:20:52.251982Z","iopub.status.idle":"2026-01-28T17:20:53.867836Z","shell.execute_reply.started":"2026-01-28T17:20:52.251962Z","shell.execute_reply":"2026-01-28T17:20:53.867207Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\nTEST_PATH  = \"/kaggle/input/datasets-avito/test-dset-small.parquet\"  \nOUT_DIR_TRAIN = \"train_featurized_parts\"   \n\nos.makedirs(OUT_DIR_TRAIN, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:53.868637Z","iopub.execute_input":"2026-01-28T17:20:53.868941Z","iopub.status.idle":"2026-01-28T17:20:53.873590Z","shell.execute_reply.started":"2026-01-28T17:20:53.868921Z","shell.execute_reply":"2026-01-28T17:20:53.872850Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"print(\"Reading schemas only...\")\ntrain_cols = pd.read_parquet(TRAIN_PATH, columns=[\"query_id\"]).shape[0]\ntest_cols  = pd.read_parquet(TEST_PATH, columns=[\"query_id\"]).shape[0]\nprint(\"train rows:\", train_cols)\nprint(\"test rows:\", test_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:53.874348Z","iopub.execute_input":"2026-01-28T17:20:53.874615Z","iopub.status.idle":"2026-01-28T17:20:54.328163Z","shell.execute_reply.started":"2026-01-28T17:20:53.874595Z","shell.execute_reply":"2026-01-28T17:20:54.327472Z"}},"outputs":[{"name":"stdout","text":"Reading schemas only...\ntrain rows: 7781790\ntest rows: 335348\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Compute global price clip...\")\nprice = pd.read_parquet(TRAIN_PATH, columns=[\"price\"])[\"price\"]\nq = price.quantile([0.999])  \nPRICE_CLIP_LOW = 0.0\nPRICE_CLIP_HIGH = float(q.loc[0.999])\nprint(\"PRICE_CLIP_HIGH =\", PRICE_CLIP_HIGH)\n\nwith open(\"global_price_clip.json\", \"w\") as f:\n    json.dump({\"PRICE_CLIP_LOW\": PRICE_CLIP_LOW, \"PRICE_CLIP_HIGH\": PRICE_CLIP_HIGH}, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:54.329984Z","iopub.execute_input":"2026-01-28T17:20:54.330218Z","iopub.status.idle":"2026-01-28T17:20:54.759077Z","shell.execute_reply.started":"2026-01-28T17:20:54.330195Z","shell.execute_reply":"2026-01-28T17:20:54.758295Z"}},"outputs":[{"name":"stdout","text":"Compute global price clip...\nPRICE_CLIP_HIGH = 22000000.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"Create qid folds...\")\nqids = pd.read_parquet(TRAIN_PATH, columns=[\"query_id\"])[\"query_id\"].drop_duplicates()\nqids = qids.sample(frac=1, random_state=42).reset_index(drop=True)\nfolds = np.array_split(qids.values, 5)\n\nfor i, arr in enumerate(folds):\n    pd.DataFrame({\"query_id\": arr}).to_parquet(f\"qids_fold_{i}.parquet\", index=False)\n\nprint(\"Saved folds sizes:\", [len(arr) for arr in folds])\nprint(\"Total unique qids:\", len(qids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:54.760108Z","iopub.execute_input":"2026-01-28T17:20:54.760514Z","iopub.status.idle":"2026-01-28T17:20:55.146812Z","shell.execute_reply.started":"2026-01-28T17:20:54.760473Z","shell.execute_reply":"2026-01-28T17:20:55.146046Z"}},"outputs":[{"name":"stdout","text":"Create qid folds...\nSaved folds sizes: [135638, 135638, 135638, 135638, 135638]\nTotal unique qids: 678190\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport gc, joblib\nfrom sklearn.feature_extraction.text import HashingVectorizer\nimport scipy.sparse as sp\n\nTRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\n\nN_FEATURES = 2**18\nNGRAM_RANGE = (1, 2)\nBATCH_SIZE_IDF = 50_000\nMAX_ROWS_IDF = None  \n\nhv = HashingVectorizer(\n    n_features=N_FEATURES,\n    alternate_sign=False,\n    norm=None,\n    lowercase=True,\n    token_pattern=r\"(?u)\\b\\w+\\b\",\n    ngram_range=NGRAM_RANGE\n)\n\ndataset = ds.dataset(TRAIN_PATH, format=\"parquet\")\nscanner = dataset.scanner(columns=[\"query_text\",\"item_title\",\"item_description\"], batch_size=BATCH_SIZE_IDF)\n\ndf_A = np.zeros(N_FEATURES, dtype=np.int64)\ndf_B = np.zeros(N_FEATURES, dtype=np.int64)\nn_docs = 0\n\ndef update_df(df_vec, X_counts):\n    present = (X_counts > 0).astype(np.int8)\n    df_vec += np.asarray(present.sum(axis=0)).ravel()\n\ntotal = 0\nfor bi, batch in enumerate(scanner.to_batches()):\n    tbl = pa.Table.from_batches([batch]).to_pandas()\n\n    qt = tbl[\"query_text\"].astype(str)\n    title = tbl[\"item_title\"].fillna(\"\").astype(str)\n    desc  = tbl[\"item_description\"].fillna(\"\").astype(str)\n\n    textA = (qt + \" \" + title).values\n    textB = (qt + \" \" + desc).values\n\n    XA = hv.transform(textA)\n    XB = hv.transform(textB)\n\n    update_df(df_A, XA)\n    update_df(df_B, XB)\n\n    n = len(tbl)\n    n_docs += n\n    total += n\n\n    del tbl, qt, title, desc, textA, textB, XA, XB\n    gc.collect()\n\n    if bi % 10 == 0:\n        print(f\"IDF batch={bi}, processed_rows={total}\")\n\n    if (MAX_ROWS_IDF is not None) and (total >= MAX_ROWS_IDF):\n        print(\"Reached MAX_ROWS_IDF, stopping.\")\n        break\n\nidf_A = np.log((1.0 + n_docs) / (1.0 + df_A)) + 1.0\nidf_B = np.log((1.0 + n_docs) / (1.0 + df_B)) + 1.0\n\njoblib.dump(hv, \"hashing_vectorizer.joblib\")\njoblib.dump(idf_A.astype(np.float32), \"idf_A.npy\")\njoblib.dump(idf_B.astype(np.float32), \"idf_B.npy\")\n\nprint(\"Saved hashing_vectorizer.joblib, idf_A.npy, idf_B.npy\")\nprint(\"n_docs:\", n_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:55.147637Z","iopub.execute_input":"2026-01-28T17:20:55.147860Z","iopub.status.idle":"2026-01-28T17:34:30.129575Z","shell.execute_reply.started":"2026-01-28T17:20:55.147840Z","shell.execute_reply":"2026-01-28T17:34:30.128862Z"}},"outputs":[{"name":"stdout","text":"IDF batch=0, processed_rows=50000\nIDF batch=10, processed_rows=550000\nIDF batch=20, processed_rows=1048576\nIDF batch=30, processed_rows=1548576\nIDF batch=40, processed_rows=2048576\nIDF batch=50, processed_rows=2547152\nIDF batch=60, processed_rows=3047152\nIDF batch=70, processed_rows=3545728\nIDF batch=80, processed_rows=4045728\nIDF batch=90, processed_rows=4544304\nIDF batch=100, processed_rows=5044304\nIDF batch=110, processed_rows=5542880\nIDF batch=120, processed_rows=6042880\nIDF batch=130, processed_rows=6541456\nIDF batch=140, processed_rows=7041456\nIDF batch=150, processed_rows=7540032\nSaved hashing_vectorizer.joblib, idf_A.npy, idf_B.npy\nn_docs: 7781790\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import numpy as np\nimport scipy.sparse as sp\n\ndef tfidf_transform_counts(X_counts_csr, idf_vec, sublinear_tf=True, l2_norm=True):\n    X = X_counts_csr.tocsr(copy=True)\n\n    if sublinear_tf:\n        X.data = np.log1p(X.data)\n\n    X = X.multiply(idf_vec)\n\n    if l2_norm:\n        row_sums = np.sqrt(X.power(2).sum(axis=1)).A1\n        row_sums[row_sums == 0] = 1.0\n        X = X.multiply(1.0 / row_sums[:, None])\n\n    return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:34:30.130376Z","iopub.execute_input":"2026-01-28T17:34:30.130687Z","iopub.status.idle":"2026-01-28T17:34:30.135687Z","shell.execute_reply.started":"2026-01-28T17:34:30.130653Z","shell.execute_reply":"2026-01-28T17:34:30.134952Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport gc, joblib\nfrom sklearn.decomposition import TruncatedSVD\n\nhv = joblib.load(\"hashing_vectorizer.joblib\")\nidf_A = joblib.load(\"idf_A.npy\")\nidf_B = joblib.load(\"idf_B.npy\")\n\nTRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\ndataset = ds.dataset(TRAIN_PATH, format=\"parquet\")\n\nBATCH_SIZE_SVD_SAMPLE = 80_000\nSVD_TRAIN_ROWS = 300_000\nN_COMP = 128\n\nscanner = dataset.scanner(columns=[\"query_text\",\"item_title\",\"item_description\"], batch_size=BATCH_SIZE_SVD_SAMPLE)\n\ntextsA_chunks, textsB_chunks = [], []\ncollected = 0\n\nfor batch in scanner.to_batches():\n    tbl = pa.Table.from_batches([batch]).to_pandas()\n    qt = tbl[\"query_text\"].astype(str)\n    title = tbl[\"item_title\"].fillna(\"\").astype(str)\n    desc  = tbl[\"item_description\"].fillna(\"\").astype(str)\n\n    textsA_chunks.append((qt + \" \" + title).values)\n    textsB_chunks.append((qt + \" \" + desc).values)\n\n    collected += len(tbl)\n    del tbl, qt, title, desc\n    gc.collect()\n\n    if collected >= SVD_TRAIN_ROWS:\n        break\n\ntextsA = np.concatenate(textsA_chunks)[:SVD_TRAIN_ROWS]\ntextsB = np.concatenate(textsB_chunks)[:SVD_TRAIN_ROWS]\ndel textsA_chunks, textsB_chunks\ngc.collect()\n\nXA_counts = hv.transform(textsA)\nXB_counts = hv.transform(textsB)\n\nXA = tfidf_transform_counts(XA_counts, idf_A, sublinear_tf=True, l2_norm=True)\nXB = tfidf_transform_counts(XB_counts, idf_B, sublinear_tf=True, l2_norm=True)\n\nsvd_A = TruncatedSVD(n_components=N_COMP, random_state=42)\nsvd_B = TruncatedSVD(n_components=N_COMP, random_state=42)\nsvd_A.fit(XA)\nsvd_B.fit(XB)\n\njoblib.dump(svd_A, \"svd_A_128.joblib\")\njoblib.dump(svd_B, \"svd_B_128.joblib\")\nprint(\"Saved SVD models\")\n\ndel textsA, textsB, XA_counts, XB_counts, XA, XB\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:34:30.136534Z","iopub.execute_input":"2026-01-28T17:34:30.136860Z","iopub.status.idle":"2026-01-28T17:37:39.613659Z","shell.execute_reply.started":"2026-01-28T17:34:30.136839Z","shell.execute_reply":"2026-01-28T17:37:39.612982Z"}},"outputs":[{"name":"stdout","text":"Saved SVD models\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import os, json\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport numpy as np\nimport pandas as pd\nimport joblib, gc\n\nTRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\nOUT_DIR_TRAIN = \"train_featurized_parts\"\nos.makedirs(OUT_DIR_TRAIN, exist_ok=True)\n\nhv = joblib.load(\"hashing_vectorizer.joblib\")\nidf_A = joblib.load(\"idf_A.npy\")\nidf_B = joblib.load(\"idf_B.npy\")\nsvd_A = joblib.load(\"svd_A_128.joblib\")\nsvd_B = joblib.load(\"svd_B_128.joblib\")\n\nwith open(\"global_price_clip.json\", \"r\") as f:\n    clip = json.load(f)\nPRICE_CLIP_LOW = clip[\"PRICE_CLIP_LOW\"]\nPRICE_CLIP_HIGH = clip[\"PRICE_CLIP_HIGH\"]\n\nBATCH_SIZE_FEAT = 30_000\n\ndataset = ds.dataset(TRAIN_PATH, format=\"parquet\")\nscanner = dataset.scanner(columns=[\n    \"query_id\",\"item_id\",\n    \"query_text\",\"item_title\",\"item_description\",\n    \"query_cat\",\"query_mcat\",\"query_loc\",\n    \"item_cat_id\",\"item_mcat_id\",\"item_loc\",\n    \"price\",\"item_query_click_conv\",\n    \"item_contact\"\n], batch_size=BATCH_SIZE_FEAT)\n\npart = 0\nrows_total = 0\n\nfor bi, batch in enumerate(scanner.to_batches()):\n    df = pa.Table.from_batches([batch]).to_pandas()\n\n    df[\"item_title\"] = df[\"item_title\"].fillna(\"\")\n    df[\"item_description\"] = df[\"item_description\"].fillna(\"\")\n    df[\"query_mcat\"] = df[\"query_mcat\"].fillna(-1)\n\n    df[\"conv_missing\"] = (df[\"item_query_click_conv\"] == -1).astype(np.int8)\n    df[\"conv_val\"] = df[\"item_query_click_conv\"].where(df[\"item_query_click_conv\"] != -1, 0).astype(np.float32)\n\n    df[\"price_clip\"] = df[\"price\"].clip(lower=PRICE_CLIP_LOW, upper=PRICE_CLIP_HIGH).astype(np.float32)\n    df[\"price_log\"]  = np.log1p(df[\"price_clip\"]).astype(np.float32)\n\n    df[\"is_loc_match\"] = (df[\"query_loc\"].astype(\"float32\") == df[\"item_loc\"].astype(\"float32\")).astype(np.int8)\n    df[\"is_cat_match\"] = (df[\"query_cat\"].astype(\"float32\") == df[\"item_cat_id\"].astype(\"float32\")).astype(np.int8)\n\n    textA = (df[\"query_text\"].astype(str) + \" \" + df[\"item_title\"].astype(str)).values\n    textB = (df[\"query_text\"].astype(str) + \" \" + df[\"item_description\"].astype(str)).values\n\n    XA_counts = hv.transform(textA)\n    XB_counts = hv.transform(textB)\n\n    XA = tfidf_transform_counts(XA_counts, idf_A, sublinear_tf=True, l2_norm=True)\n    XB = tfidf_transform_counts(XB_counts, idf_B, sublinear_tf=True, l2_norm=True)\n\n    ZA = svd_A.transform(XA).astype(np.float32)\n    ZB = svd_B.transform(XB).astype(np.float32)\n\n    ZA_df = pd.DataFrame(ZA, columns=[f\"tfidfA_svd_{j}\" for j in range(ZA.shape[1])])\n    ZB_df = pd.DataFrame(ZB, columns=[f\"tfidfB_svd_{j}\" for j in range(ZB.shape[1])])\n\n    df = pd.concat([df.reset_index(drop=True), ZA_df, ZB_df], axis=1)\n\n    df = df.drop(columns=[\"query_text\",\"item_title\",\"item_description\",\"price\",\"item_query_click_conv\"])\n\n    out_path = f\"{OUT_DIR_TRAIN}/part_{part:03d}.parquet\"\n    df.to_parquet(out_path, index=False)\n\n    rows_total += len(df)\n    part += 1\n\n    del df, textA, textB, XA_counts, XB_counts, XA, XB, ZA, ZB\n    gc.collect()\n\n    if bi % 10 == 0:\n        print(f\"FEAT batch={bi}, saved_parts={part}, rows_total={rows_total}\")\n\nprint(\"DONE. parts:\", part, \"rows:\", rows_total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:37:39.614734Z","iopub.execute_input":"2026-01-28T17:37:39.615314Z","iopub.status.idle":"2026-01-28T18:01:37.958529Z","shell.execute_reply.started":"2026-01-28T17:37:39.615290Z","shell.execute_reply":"2026-01-28T18:01:37.957699Z"}},"outputs":[{"name":"stdout","text":"FEAT batch=0, saved_parts=1, rows_total=30000\nFEAT batch=10, saved_parts=11, rows_total=330000\nFEAT batch=20, saved_parts=21, rows_total=630000\nFEAT batch=30, saved_parts=31, rows_total=930000\nFEAT batch=40, saved_parts=41, rows_total=1228576\nFEAT batch=50, saved_parts=51, rows_total=1528576\nFEAT batch=60, saved_parts=61, rows_total=1828576\nFEAT batch=70, saved_parts=71, rows_total=2127152\nFEAT batch=80, saved_parts=81, rows_total=2427152\nFEAT batch=90, saved_parts=91, rows_total=2727152\nFEAT batch=100, saved_parts=101, rows_total=3027152\nFEAT batch=110, saved_parts=111, rows_total=3325728\nFEAT batch=120, saved_parts=121, rows_total=3625728\nFEAT batch=130, saved_parts=131, rows_total=3925728\nFEAT batch=140, saved_parts=141, rows_total=4224304\nFEAT batch=150, saved_parts=151, rows_total=4524304\nFEAT batch=160, saved_parts=161, rows_total=4824304\nFEAT batch=170, saved_parts=171, rows_total=5124304\nFEAT batch=180, saved_parts=181, rows_total=5422880\nFEAT batch=190, saved_parts=191, rows_total=5722880\nFEAT batch=200, saved_parts=201, rows_total=6022880\nFEAT batch=210, saved_parts=211, rows_total=6321456\nFEAT batch=220, saved_parts=221, rows_total=6621456\nFEAT batch=230, saved_parts=231, rows_total=6921456\nFEAT batch=240, saved_parts=241, rows_total=7221456\nFEAT batch=250, saved_parts=251, rows_total=7520032\nDONE. parts: 260 rows: 7781790\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Preprocessing TF-IDF (Test)","metadata":{}},{"cell_type":"code","source":"import os, gc, json\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport joblib\nimport scipy.sparse as sp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:01:37.959481Z","iopub.execute_input":"2026-01-28T18:01:37.959824Z","iopub.status.idle":"2026-01-28T18:01:37.963746Z","shell.execute_reply.started":"2026-01-28T18:01:37.959800Z","shell.execute_reply":"2026-01-28T18:01:37.963012Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"TEST_PATH = \"/kaggle/input/datasets-avito/test-dset-small.parquet\"\nOUT_DIR_TEST = \"test_featurized_parts\"\nos.makedirs(OUT_DIR_TEST, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:01:37.964572Z","iopub.execute_input":"2026-01-28T18:01:37.964813Z","iopub.status.idle":"2026-01-28T18:01:37.976104Z","shell.execute_reply.started":"2026-01-28T18:01:37.964785Z","shell.execute_reply":"2026-01-28T18:01:37.975449Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"hv = joblib.load(\"hashing_vectorizer.joblib\")\nidf_A = joblib.load(\"idf_A.npy\")              \nidf_B = joblib.load(\"idf_B.npy\")              \nsvd_A = joblib.load(\"svd_A_128.joblib\")\nsvd_B = joblib.load(\"svd_B_128.joblib\")\n\nwith open(\"global_price_clip.json\", \"r\") as f:\n    clip = json.load(f)\nPRICE_CLIP_LOW = float(clip[\"PRICE_CLIP_LOW\"])\nPRICE_CLIP_HIGH = float(clip[\"PRICE_CLIP_HIGH\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:01:37.976988Z","iopub.execute_input":"2026-01-28T18:01:37.977799Z","iopub.status.idle":"2026-01-28T18:01:40.449633Z","shell.execute_reply.started":"2026-01-28T18:01:37.977778Z","shell.execute_reply":"2026-01-28T18:01:40.449048Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def tfidf_transform_counts(X_counts_csr, idf_vec, sublinear_tf=True, l2_norm=True):\n    X = X_counts_csr.tocsr(copy=True)\n    if sublinear_tf:\n        X.data = np.log1p(X.data)\n    X = X.multiply(idf_vec)\n    if l2_norm:\n        row_sums = np.sqrt(X.power(2).sum(axis=1)).A1\n        row_sums[row_sums == 0] = 1.0\n        X = X.multiply(1.0 / row_sums[:, None])\n    return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:01:40.451813Z","iopub.execute_input":"2026-01-28T18:01:40.452049Z","iopub.status.idle":"2026-01-28T18:01:40.456602Z","shell.execute_reply.started":"2026-01-28T18:01:40.452029Z","shell.execute_reply":"2026-01-28T18:01:40.455893Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"BATCH_SIZE = 30_000  \n\ndataset = ds.dataset(TEST_PATH, format=\"parquet\")\nscanner = dataset.scanner(columns=[\n    \"query_id\",\"item_id\",\n    \"query_text\",\"item_title\",\"item_description\",\n    \"query_cat\",\"query_mcat\",\"query_loc\",\n    \"item_cat_id\",\"item_mcat_id\",\"item_loc\",\n    \"price\",\"item_query_click_conv\",\n], batch_size=BATCH_SIZE)\n\npart = 0\nrows_total = 0\n\nfor bi, batch in enumerate(scanner.to_batches()):\n    df = pa.Table.from_batches([batch]).to_pandas()\n\n    df[\"item_title\"] = df[\"item_title\"].fillna(\"\")\n    df[\"item_description\"] = df[\"item_description\"].fillna(\"\")\n    df[\"query_mcat\"] = df[\"query_mcat\"].fillna(-1)\n\n    df[\"conv_missing\"] = (df[\"item_query_click_conv\"] == -1).astype(np.int8)\n    df[\"conv_val\"] = df[\"item_query_click_conv\"].where(df[\"item_query_click_conv\"] != -1, 0).astype(np.float32)\n\n    df[\"price_clip\"] = df[\"price\"].clip(lower=PRICE_CLIP_LOW, upper=PRICE_CLIP_HIGH).astype(np.float32)\n    df[\"price_log\"]  = np.log1p(df[\"price_clip\"]).astype(np.float32)\n\n    df[\"is_loc_match\"] = (df[\"query_loc\"].astype(\"float32\") == df[\"item_loc\"].astype(\"float32\")).astype(np.int8)\n    df[\"is_cat_match\"] = (df[\"query_cat\"].astype(\"float32\") == df[\"item_cat_id\"].astype(\"float32\")).astype(np.int8)\n\n    textA = (df[\"query_text\"].astype(str) + \" \" + df[\"item_title\"].astype(str)).values\n    textB = (df[\"query_text\"].astype(str) + \" \" + df[\"item_description\"].astype(str)).values\n\n    XA_counts = hv.transform(textA)\n    XB_counts = hv.transform(textB)\n\n    XA = tfidf_transform_counts(XA_counts, idf_A, sublinear_tf=True, l2_norm=True)\n    XB = tfidf_transform_counts(XB_counts, idf_B, sublinear_tf=True, l2_norm=True)\n\n    ZA = svd_A.transform(XA).astype(np.float32)\n    ZB = svd_B.transform(XB).astype(np.float32)\n\n    ZA_df = pd.DataFrame(ZA, columns=[f\"tfidfA_svd_{j}\" for j in range(ZA.shape[1])])\n    ZB_df = pd.DataFrame(ZB, columns=[f\"tfidfB_svd_{j}\" for j in range(ZB.shape[1])])\n    df = pd.concat([df.reset_index(drop=True), ZA_df, ZB_df], axis=1)\n\n    df = df.drop(columns=[\"query_text\",\"item_title\",\"item_description\",\"price\",\"item_query_click_conv\"])\n\n    out_path = f\"{OUT_DIR_TEST}/part_{part:03d}.parquet\"\n    df.to_parquet(out_path, index=False)\n\n    rows_total += len(df)\n    part += 1\n\n    del df, ZA, ZB, ZA_df, ZB_df, XA_counts, XB_counts, XA, XB, textA, textB\n    gc.collect()\n\n    if bi % 10 == 0:\n        print(f\"TEST batch={bi}, saved_parts={part}, rows_total={rows_total}\")\n\nprint(\"DONE. test parts:\", part, \"rows:\", rows_total)\nprint(\"Output dir:\", OUT_DIR_TEST)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:01:40.457535Z","iopub.execute_input":"2026-01-28T18:01:40.457868Z","iopub.status.idle":"2026-01-28T18:02:48.087143Z","shell.execute_reply.started":"2026-01-28T18:01:40.457839Z","shell.execute_reply":"2026-01-28T18:02:48.086420Z"}},"outputs":[{"name":"stdout","text":"TEST batch=0, saved_parts=1, rows_total=30000\nTEST batch=10, saved_parts=11, rows_total=330000\nDONE. test parts: 12 rows: 335348\nOutput dir: test_featurized_parts\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import glob, gc\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostRanker, Pool","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:02:48.088103Z","iopub.execute_input":"2026-01-28T18:02:48.088486Z","iopub.status.idle":"2026-01-28T18:02:48.766038Z","shell.execute_reply.started":"2026-01-28T18:02:48.088453Z","shell.execute_reply":"2026-01-28T18:02:48.765460Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"PARTS_DIR = \"train_featurized_parts\"\nparts = sorted(glob.glob(f\"{PARTS_DIR}/part_*.parquet\"))\nprint(\"n_parts:\", len(parts))\n\nval_qids = set(pd.read_parquet(\"qids_fold_0.parquet\")[\"query_id\"].values)\n\ntrain_qids = set(pd.read_parquet(\"qids_fold_1.parquet\")[\"query_id\"].values) | \\\n             set(pd.read_parquet(\"qids_fold_2.parquet\")[\"query_id\"].values)\n\nprint(\"val_qids:\", len(val_qids), \"train_qids:\", len(train_qids))\n\ntrain_chunks, val_chunks = [], []\n\nfor i, p in enumerate(parts):\n    df = pd.read_parquet(p)\n\n    m_val = df[\"query_id\"].isin(val_qids)\n    if m_val.any():\n        val_chunks.append(df[m_val])\n\n    m_tr = df[\"query_id\"].isin(train_qids)\n    if m_tr.any():\n        train_chunks.append(df[m_tr])\n\n    if i % 20 == 0:\n        tr_rows = sum(len(x) for x in train_chunks)\n        va_rows = sum(len(x) for x in val_chunks)\n        print(f\"processed {i}/{len(parts)} | tr_rows={tr_rows} va_rows={va_rows}\")\n\n    del df\n    gc.collect()\n\ntr = pd.concat(train_chunks, ignore_index=True)\nva = pd.concat(val_chunks, ignore_index=True)\ndel train_chunks, val_chunks\ngc.collect()\n\nprint(\"FINAL tr shape:\", tr.shape, \"va shape:\", va.shape)\nprint(\"tr target mean:\", tr[\"item_contact\"].mean(), \"va target mean:\", va[\"item_contact\"].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:02:48.767002Z","iopub.execute_input":"2026-01-28T18:02:48.767444Z","iopub.status.idle":"2026-01-28T18:04:43.406358Z","shell.execute_reply.started":"2026-01-28T18:02:48.767389Z","shell.execute_reply":"2026-01-28T18:04:43.405676Z"}},"outputs":[{"name":"stdout","text":"n_parts: 260\nval_qids: 135638 train_qids: 271276\nprocessed 0/260 | tr_rows=12635 va_rows=5473\nprocessed 20/260 | tr_rows=251457 va_rows=124516\nprocessed 40/260 | tr_rows=490909 va_rows=242458\nprocessed 60/260 | tr_rows=730689 va_rows=361931\nprocessed 80/260 | tr_rows=970499 va_rows=482273\nprocessed 100/260 | tr_rows=1210114 va_rows=603283\nprocessed 120/260 | tr_rows=1446432 va_rows=722355\nprocessed 140/260 | tr_rows=1687490 va_rows=841704\nprocessed 160/260 | tr_rows=1927545 va_rows=961651\nprocessed 180/260 | tr_rows=2165897 va_rows=1081899\nprocessed 200/260 | tr_rows=2407558 va_rows=1202363\nprocessed 220/260 | tr_rows=2645871 va_rows=1322770\nprocessed 240/260 | tr_rows=2887513 va_rows=1443137\nFINAL tr shape: (3112699, 271) va shape: (1556058, 271)\ntr target mean: 0.04416553 va target mean: 0.043723308\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"TARGET = \"item_contact\"\nGROUP = \"query_id\"\ndrop_cols = [TARGET, \"item_id\", \"query_id\"]\nfeatures = [c for c in tr.columns if c not in drop_cols]\n\nprint(\"n_features:\", len(features))\n\ntr = tr.sort_values(GROUP, kind=\"mergesort\").reset_index(drop=True)\nva = va.sort_values(GROUP, kind=\"mergesort\").reset_index(drop=True)\n\ncat_features = []\n\ntrain_pool = Pool(\n    tr[features],\n    label=tr[TARGET].astype(int),\n    group_id=tr[GROUP].values,\n    cat_features=cat_features\n)\nvalid_pool = Pool(\n    va[features],\n    label=va[TARGET].astype(int),\n    group_id=va[GROUP].values,\n    cat_features=cat_features\n)\n\nranker = CatBoostRanker(\n    loss_function=\"YetiRank\",\n    eval_metric=\"NDCG:top=10\",\n    iterations=3000,\n    learning_rate=0.05,\n    depth=8,\n    random_seed=42,\n    verbose=100,\n    task_type=\"GPU\"\n)\n\nranker.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n\nprint(\"best_iteration:\", ranker.get_best_iteration())\nprint(\"best_score:\", ranker.get_best_score())\n\nranker.save_model(\"catboost_ranker_40pct.cbm\")\nprint(\"Saved model: catboost_ranker_40pct.cbm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:04:43.409085Z","iopub.execute_input":"2026-01-28T18:04:43.409315Z","iopub.status.idle":"2026-01-28T18:11:54.103334Z","shell.execute_reply.started":"2026-01-28T18:04:43.409293Z","shell.execute_reply":"2026-01-28T18:11:54.102354Z"}},"outputs":[{"name":"stdout","text":"n_features: 268\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because PFound, NDCG is/are not implemented for GPU\nMetric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\nMetric NDCG:top=10;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.7968169\tbest: 0.7968169 (0)\ttotal: 515ms\tremaining: 25m 44s\n100:\ttest: 0.8135200\tbest: 0.8135200 (100)\ttotal: 12.7s\tremaining: 6m 5s\n200:\ttest: 0.8142720\tbest: 0.8142720 (200)\ttotal: 24.9s\tremaining: 5m 46s\n300:\ttest: 0.8149713\tbest: 0.8149713 (300)\ttotal: 36.9s\tremaining: 5m 31s\n400:\ttest: 0.8155117\tbest: 0.8155117 (400)\ttotal: 49s\tremaining: 5m 17s\n500:\ttest: 0.8159535\tbest: 0.8159593 (490)\ttotal: 1m 1s\tremaining: 5m 4s\n600:\ttest: 0.8163110\tbest: 0.8163110 (600)\ttotal: 1m 13s\tremaining: 4m 52s\n700:\ttest: 0.8165800\tbest: 0.8165800 (700)\ttotal: 1m 25s\tremaining: 4m 41s\n800:\ttest: 0.8166666\tbest: 0.8167175 (790)\ttotal: 1m 38s\tremaining: 4m 29s\n900:\ttest: 0.8169356\tbest: 0.8169356 (900)\ttotal: 1m 50s\tremaining: 4m 17s\n1000:\ttest: 0.8170393\tbest: 0.8171149 (985)\ttotal: 2m 3s\tremaining: 4m 6s\n1100:\ttest: 0.8171425\tbest: 0.8171739 (1090)\ttotal: 2m 16s\tremaining: 3m 54s\n1200:\ttest: 0.8171862\tbest: 0.8172302 (1175)\ttotal: 2m 29s\tremaining: 3m 43s\n1300:\ttest: 0.8172736\tbest: 0.8173052 (1235)\ttotal: 2m 42s\tremaining: 3m 31s\n1400:\ttest: 0.8173936\tbest: 0.8173936 (1400)\ttotal: 2m 55s\tremaining: 3m 20s\n1500:\ttest: 0.8174763\tbest: 0.8174840 (1490)\ttotal: 3m 8s\tremaining: 3m 7s\n1600:\ttest: 0.8174627\tbest: 0.8175092 (1585)\ttotal: 3m 21s\tremaining: 2m 55s\n1700:\ttest: 0.8177116\tbest: 0.8177116 (1700)\ttotal: 3m 33s\tremaining: 2m 43s\n1800:\ttest: 0.8177223\tbest: 0.8178428 (1735)\ttotal: 3m 46s\tremaining: 2m 30s\n1900:\ttest: 0.8177901\tbest: 0.8178428 (1735)\ttotal: 3m 59s\tremaining: 2m 18s\n2000:\ttest: 0.8178221\tbest: 0.8178428 (1735)\ttotal: 4m 12s\tremaining: 2m 6s\n2100:\ttest: 0.8178119\tbest: 0.8178428 (1735)\ttotal: 4m 25s\tremaining: 1m 53s\n2200:\ttest: 0.8177924\tbest: 0.8178428 (1735)\ttotal: 4m 38s\tremaining: 1m 41s\n2300:\ttest: 0.8177842\tbest: 0.8178479 (2220)\ttotal: 4m 51s\tremaining: 1m 28s\n2400:\ttest: 0.8178091\tbest: 0.8178786 (2370)\ttotal: 5m 4s\tremaining: 1m 15s\n2500:\ttest: 0.8177142\tbest: 0.8178786 (2370)\ttotal: 5m 16s\tremaining: 1m 3s\n2600:\ttest: 0.8178542\tbest: 0.8178786 (2370)\ttotal: 5m 29s\tremaining: 50.6s\n2700:\ttest: 0.8178516\tbest: 0.8179281 (2620)\ttotal: 5m 43s\tremaining: 38s\n2800:\ttest: 0.8178606\tbest: 0.8179281 (2620)\ttotal: 5m 56s\tremaining: 25.3s\n2900:\ttest: 0.8179806\tbest: 0.8179806 (2900)\ttotal: 6m 9s\tremaining: 12.6s\n2999:\ttest: 0.8178393\tbest: 0.8179806 (2900)\ttotal: 6m 22s\tremaining: 0us\nbestTest = 0.8179806474\nbestIteration = 2900\nShrink model to first 2901 iterations.\nbest_iteration: 2900\nbest_score: {}\nSaved model: catboost_ranker_40pct.cbm\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Predict + Submission","metadata":{}},{"cell_type":"code","source":"import glob, gc\nimport pandas as pd\nfrom catboost import CatBoostRanker","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:11:54.104444Z","iopub.execute_input":"2026-01-28T18:11:54.104708Z","iopub.status.idle":"2026-01-28T18:11:54.109307Z","shell.execute_reply.started":"2026-01-28T18:11:54.104679Z","shell.execute_reply":"2026-01-28T18:11:54.108644Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"ranker = CatBoostRanker()\nranker.load_model(\"catboost_ranker_40pct.cbm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:11:54.109920Z","iopub.execute_input":"2026-01-28T18:11:54.110140Z","iopub.status.idle":"2026-01-28T18:11:54.135665Z","shell.execute_reply.started":"2026-01-28T18:11:54.110117Z","shell.execute_reply":"2026-01-28T18:11:54.134982Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<catboost.core.CatBoostRanker at 0x7925a0517b30>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"PARTS_DIR_TEST = \"test_featurized_parts\"\ntest_parts = sorted(glob.glob(f\"{PARTS_DIR_TEST}/part_*.parquet\"))\nprint(\"n_test_parts:\", len(test_parts))\n\npred_parts = []\ntotal = 0\n\nfor i, p in enumerate(test_parts):\n    df = pd.read_parquet(p)\n\n    feature_cols = [c for c in df.columns if c not in [\"query_id\", \"item_id\"]]\n    df[\"score\"] = ranker.predict(df[feature_cols])\n\n    pred_parts.append(df[[\"query_id\", \"item_id\", \"score\"]])\n    total += len(df)\n\n    del df\n    gc.collect()\n\n    print(f\"pred {i+1}/{len(test_parts)} total_rows={total}\")\n\npred = pd.concat(pred_parts, ignore_index=True)\ndel pred_parts\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:11:54.136489Z","iopub.execute_input":"2026-01-28T18:11:54.136729Z","iopub.status.idle":"2026-01-28T18:11:59.059349Z","shell.execute_reply.started":"2026-01-28T18:11:54.136704Z","shell.execute_reply":"2026-01-28T18:11:59.058788Z"}},"outputs":[{"name":"stdout","text":"n_test_parts: 12\npred 1/12 total_rows=30000\npred 2/12 total_rows=60000\npred 3/12 total_rows=90000\npred 4/12 total_rows=120000\npred 5/12 total_rows=150000\npred 6/12 total_rows=180000\npred 7/12 total_rows=210000\npred 8/12 total_rows=240000\npred 9/12 total_rows=270000\npred 10/12 total_rows=300000\npred 11/12 total_rows=330000\npred 12/12 total_rows=335348\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"submission_df = pred.sort_values([\"query_id\", \"score\"], ascending=[True, False])[[\"query_id\", \"item_id\"]]\nsubmission_df.to_csv(\"solution.csv\", header=[\"query_id\", \"item_id\"], index=False)\n\nprint(\"Saved solution.csv shape:\", submission_df.shape)\nprint(submission_df.head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:11:59.060249Z","iopub.execute_input":"2026-01-28T18:11:59.060503Z","iopub.status.idle":"2026-01-28T18:11:59.485514Z","shell.execute_reply.started":"2026-01-28T18:11:59.060482Z","shell.execute_reply":"2026-01-28T18:11:59.484783Z"}},"outputs":[{"name":"stdout","text":"Saved solution.csv shape: (335348, 2)\n    query_id     item_id\n4         55  7552455685\n15        55   823036541\n11        55  7464296355\n19        55  3635185843\n14        55  7555956997\n22        55  7562354327\n28        55  7489404885\n8         55  7552064016\n34        55  7549689548\n27        55  7576666895\n24        55  2427651981\n3         55  7587733901\n30        55  4600495891\n31        55  7568390402\n21        55  2231316338\n26        55  7585975325\n29        55  7377513020\n6         55  3298429910\n0         55  7540855789\n18        55  2499344704\n","output_type":"stream"}],"execution_count":23}]}