{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14593623,"sourceType":"datasetVersion","datasetId":9321923}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:13:55.825918Z","iopub.execute_input":"2026-01-27T04:13:55.826193Z","iopub.status.idle":"2026-01-27T04:13:57.166491Z","shell.execute_reply.started":"2026-01-27T04:13:55.826168Z","shell.execute_reply":"2026-01-27T04:13:57.165704Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets-avito/train-dset.parquet\n/kaggle/input/datasets-avito/test-dset-small.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Preprocessing + TF-IDF (Train)","metadata":{}},{"cell_type":"code","source":"import os, gc, json\nimport numpy as np\nimport pandas as pd\n\nimport pyarrow as pa\nimport pyarrow.dataset as ds\n\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:13:57.168179Z","iopub.execute_input":"2026-01-27T04:13:57.168608Z","iopub.status.idle":"2026-01-27T04:13:59.145488Z","shell.execute_reply.started":"2026-01-27T04:13:57.168582Z","shell.execute_reply":"2026-01-27T04:13:59.144577Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\nTEST_PATH  = \"/kaggle/input/datasets-avito/test-dset-small.parquet\"  \nOUT_DIR_TRAIN = \"train_featurized_parts\"   \n\nos.makedirs(OUT_DIR_TRAIN, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:13:59.146615Z","iopub.execute_input":"2026-01-27T04:13:59.147017Z","iopub.status.idle":"2026-01-27T04:13:59.151207Z","shell.execute_reply.started":"2026-01-27T04:13:59.146991Z","shell.execute_reply":"2026-01-27T04:13:59.150553Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"Reading schemas only...\")\ntrain_cols = pd.read_parquet(TRAIN_PATH, columns=[\"query_id\"]).shape[0]\ntest_cols  = pd.read_parquet(TEST_PATH, columns=[\"query_id\"]).shape[0]\nprint(\"train rows:\", train_cols)\nprint(\"test rows:\", test_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:13:59.152111Z","iopub.execute_input":"2026-01-27T04:13:59.152304Z","iopub.status.idle":"2026-01-27T04:13:59.681145Z","shell.execute_reply.started":"2026-01-27T04:13:59.152284Z","shell.execute_reply":"2026-01-27T04:13:59.680477Z"}},"outputs":[{"name":"stdout","text":"Reading schemas only...\ntrain rows: 7781790\ntest rows: 335348\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"Compute global price clip...\")\nprice = pd.read_parquet(TRAIN_PATH, columns=[\"price\"])[\"price\"]\nq = price.quantile([0.999])  \nPRICE_CLIP_LOW = 0.0\nPRICE_CLIP_HIGH = float(q.loc[0.999])\nprint(\"PRICE_CLIP_HIGH =\", PRICE_CLIP_HIGH)\n\nwith open(\"global_price_clip.json\", \"w\") as f:\n    json.dump({\"PRICE_CLIP_LOW\": PRICE_CLIP_LOW, \"PRICE_CLIP_HIGH\": PRICE_CLIP_HIGH}, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:13:59.682040Z","iopub.execute_input":"2026-01-27T04:13:59.682412Z","iopub.status.idle":"2026-01-27T04:14:00.135471Z","shell.execute_reply.started":"2026-01-27T04:13:59.682384Z","shell.execute_reply":"2026-01-27T04:14:00.134458Z"}},"outputs":[{"name":"stdout","text":"\n[1] Compute global price clip...\nPRICE_CLIP_HIGH = 22000000.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Create qid folds...\")\nqids = pd.read_parquet(TRAIN_PATH, columns=[\"query_id\"])[\"query_id\"].drop_duplicates()\nqids = qids.sample(frac=1, random_state=42).reset_index(drop=True)\nfolds = np.array_split(qids.values, 5)\n\nfor i, arr in enumerate(folds):\n    pd.DataFrame({\"query_id\": arr}).to_parquet(f\"qids_fold_{i}.parquet\", index=False)\n\nprint(\"Saved folds sizes:\", [len(arr) for arr in folds])\nprint(\"Total unique qids:\", len(qids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:14:00.138146Z","iopub.execute_input":"2026-01-27T04:14:00.138507Z","iopub.status.idle":"2026-01-27T04:14:00.553122Z","shell.execute_reply.started":"2026-01-27T04:14:00.138481Z","shell.execute_reply":"2026-01-27T04:14:00.552363Z"}},"outputs":[{"name":"stdout","text":"\n[2] Create qid folds...\nSaved folds sizes: [135638, 135638, 135638, 135638, 135638]\nTotal unique qids: 678190\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport gc, joblib\nfrom sklearn.feature_extraction.text import HashingVectorizer\nimport scipy.sparse as sp\n\nTRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\n\nN_FEATURES = 2**18\nNGRAM_RANGE = (1, 2)\nBATCH_SIZE_IDF = 50_000\nMAX_ROWS_IDF = None  \n\nhv = HashingVectorizer(\n    n_features=N_FEATURES,\n    alternate_sign=False,\n    norm=None,\n    lowercase=True,\n    token_pattern=r\"(?u)\\b\\w+\\b\",\n    ngram_range=NGRAM_RANGE\n)\n\ndataset = ds.dataset(TRAIN_PATH, format=\"parquet\")\nscanner = dataset.scanner(columns=[\"query_text\",\"item_title\",\"item_description\"], batch_size=BATCH_SIZE_IDF)\n\ndf_A = np.zeros(N_FEATURES, dtype=np.int64)\ndf_B = np.zeros(N_FEATURES, dtype=np.int64)\nn_docs = 0\n\ndef update_df(df_vec, X_counts):\n    present = (X_counts > 0).astype(np.int8)\n    df_vec += np.asarray(present.sum(axis=0)).ravel()\n\ntotal = 0\nfor bi, batch in enumerate(scanner.to_batches()):\n    tbl = pa.Table.from_batches([batch]).to_pandas()\n\n    qt = tbl[\"query_text\"].astype(str)\n    title = tbl[\"item_title\"].fillna(\"\").astype(str)\n    desc  = tbl[\"item_description\"].fillna(\"\").astype(str)\n\n    textA = (qt + \" \" + title).values\n    textB = (qt + \" \" + desc).values\n\n    XA = hv.transform(textA)\n    XB = hv.transform(textB)\n\n    update_df(df_A, XA)\n    update_df(df_B, XB)\n\n    n = len(tbl)\n    n_docs += n\n    total += n\n\n    del tbl, qt, title, desc, textA, textB, XA, XB\n    gc.collect()\n\n    if bi % 10 == 0:\n        print(f\"IDF batch={bi}, processed_rows={total}\")\n\n    if (MAX_ROWS_IDF is not None) and (total >= MAX_ROWS_IDF):\n        print(\"Reached MAX_ROWS_IDF, stopping.\")\n        break\n\nidf_A = np.log((1.0 + n_docs) / (1.0 + df_A)) + 1.0\nidf_B = np.log((1.0 + n_docs) / (1.0 + df_B)) + 1.0\n\njoblib.dump(hv, \"hashing_vectorizer.joblib\")\njoblib.dump(idf_A.astype(np.float32), \"idf_A.npy\")\njoblib.dump(idf_B.astype(np.float32), \"idf_B.npy\")\n\nprint(\"Saved hashing_vectorizer.joblib, idf_A.npy, idf_B.npy\")\nprint(\"n_docs:\", n_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:14:00.554298Z","iopub.execute_input":"2026-01-27T04:14:00.554743Z","iopub.status.idle":"2026-01-27T04:28:56.120740Z","shell.execute_reply.started":"2026-01-27T04:14:00.554716Z","shell.execute_reply":"2026-01-27T04:28:56.120061Z"}},"outputs":[{"name":"stdout","text":"IDF batch=0, processed_rows=50000\nIDF batch=10, processed_rows=550000\nIDF batch=20, processed_rows=1048576\nIDF batch=30, processed_rows=1548576\nIDF batch=40, processed_rows=2048576\nIDF batch=50, processed_rows=2547152\nIDF batch=60, processed_rows=3047152\nIDF batch=70, processed_rows=3545728\nIDF batch=80, processed_rows=4045728\nIDF batch=90, processed_rows=4544304\nIDF batch=100, processed_rows=5044304\nIDF batch=110, processed_rows=5542880\nIDF batch=120, processed_rows=6042880\nIDF batch=130, processed_rows=6541456\nIDF batch=140, processed_rows=7041456\nIDF batch=150, processed_rows=7540032\nSaved hashing_vectorizer.joblib, idf_A.npy, idf_B.npy\nn_docs: 7781790\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport scipy.sparse as sp\n\ndef tfidf_transform_counts(X_counts_csr, idf_vec, sublinear_tf=True, l2_norm=True):\n    X = X_counts_csr.tocsr(copy=True)\n\n    if sublinear_tf:\n        X.data = np.log1p(X.data)\n\n    X = X.multiply(idf_vec)\n\n    if l2_norm:\n        row_sums = np.sqrt(X.power(2).sum(axis=1)).A1\n        row_sums[row_sums == 0] = 1.0\n        X = X.multiply(1.0 / row_sums[:, None])\n\n    return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:28:56.121717Z","iopub.execute_input":"2026-01-27T04:28:56.121994Z","iopub.status.idle":"2026-01-27T04:28:56.127399Z","shell.execute_reply.started":"2026-01-27T04:28:56.121970Z","shell.execute_reply":"2026-01-27T04:28:56.126738Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import numpy as np\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport gc, joblib\nfrom sklearn.decomposition import TruncatedSVD\n\nhv = joblib.load(\"hashing_vectorizer.joblib\")\nidf_A = joblib.load(\"idf_A.npy\")\nidf_B = joblib.load(\"idf_B.npy\")\n\nTRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\ndataset = ds.dataset(TRAIN_PATH, format=\"parquet\")\n\nBATCH_SIZE_SVD_SAMPLE = 80_000\nSVD_TRAIN_ROWS = 300_000\nN_COMP = 128\n\nscanner = dataset.scanner(columns=[\"query_text\",\"item_title\",\"item_description\"], batch_size=BATCH_SIZE_SVD_SAMPLE)\n\ntextsA_chunks, textsB_chunks = [], []\ncollected = 0\n\nfor batch in scanner.to_batches():\n    tbl = pa.Table.from_batches([batch]).to_pandas()\n    qt = tbl[\"query_text\"].astype(str)\n    title = tbl[\"item_title\"].fillna(\"\").astype(str)\n    desc  = tbl[\"item_description\"].fillna(\"\").astype(str)\n\n    textsA_chunks.append((qt + \" \" + title).values)\n    textsB_chunks.append((qt + \" \" + desc).values)\n\n    collected += len(tbl)\n    del tbl, qt, title, desc\n    gc.collect()\n\n    if collected >= SVD_TRAIN_ROWS:\n        break\n\ntextsA = np.concatenate(textsA_chunks)[:SVD_TRAIN_ROWS]\ntextsB = np.concatenate(textsB_chunks)[:SVD_TRAIN_ROWS]\ndel textsA_chunks, textsB_chunks\ngc.collect()\n\nXA_counts = hv.transform(textsA)\nXB_counts = hv.transform(textsB)\n\nXA = tfidf_transform_counts(XA_counts, idf_A, sublinear_tf=True, l2_norm=True)\nXB = tfidf_transform_counts(XB_counts, idf_B, sublinear_tf=True, l2_norm=True)\n\nsvd_A = TruncatedSVD(n_components=N_COMP, random_state=42)\nsvd_B = TruncatedSVD(n_components=N_COMP, random_state=42)\nsvd_A.fit(XA)\nsvd_B.fit(XB)\n\njoblib.dump(svd_A, \"svd_A_128.joblib\")\njoblib.dump(svd_B, \"svd_B_128.joblib\")\nprint(\"Saved SVD models\")\n\ndel textsA, textsB, XA_counts, XB_counts, XA, XB\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:28:56.128209Z","iopub.execute_input":"2026-01-27T04:28:56.128475Z","iopub.status.idle":"2026-01-27T04:32:18.070636Z","shell.execute_reply.started":"2026-01-27T04:28:56.128452Z","shell.execute_reply":"2026-01-27T04:32:18.069852Z"}},"outputs":[{"name":"stdout","text":"Saved SVD models\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import os, json\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport numpy as np\nimport pandas as pd\nimport joblib, gc\n\nTRAIN_PATH = \"/kaggle/input/datasets-avito/train-dset.parquet\"\nOUT_DIR_TRAIN = \"train_featurized_parts\"\nos.makedirs(OUT_DIR_TRAIN, exist_ok=True)\n\nhv = joblib.load(\"hashing_vectorizer.joblib\")\nidf_A = joblib.load(\"idf_A.npy\")\nidf_B = joblib.load(\"idf_B.npy\")\nsvd_A = joblib.load(\"svd_A_128.joblib\")\nsvd_B = joblib.load(\"svd_B_128.joblib\")\n\nwith open(\"global_price_clip.json\", \"r\") as f:\n    clip = json.load(f)\nPRICE_CLIP_LOW = clip[\"PRICE_CLIP_LOW\"]\nPRICE_CLIP_HIGH = clip[\"PRICE_CLIP_HIGH\"]\n\nBATCH_SIZE_FEAT = 30_000\n\ndataset = ds.dataset(TRAIN_PATH, format=\"parquet\")\nscanner = dataset.scanner(columns=[\n    \"query_id\",\"item_id\",\n    \"query_text\",\"item_title\",\"item_description\",\n    \"query_cat\",\"query_mcat\",\"query_loc\",\n    \"item_cat_id\",\"item_mcat_id\",\"item_loc\",\n    \"price\",\"item_query_click_conv\",\n    \"item_contact\"\n], batch_size=BATCH_SIZE_FEAT)\n\npart = 0\nrows_total = 0\n\nfor bi, batch in enumerate(scanner.to_batches()):\n    df = pa.Table.from_batches([batch]).to_pandas()\n\n    df[\"item_title\"] = df[\"item_title\"].fillna(\"\")\n    df[\"item_description\"] = df[\"item_description\"].fillna(\"\")\n    df[\"query_mcat\"] = df[\"query_mcat\"].fillna(-1)\n\n    df[\"conv_missing\"] = (df[\"item_query_click_conv\"] == -1).astype(np.int8)\n    df[\"conv_val\"] = df[\"item_query_click_conv\"].where(df[\"item_query_click_conv\"] != -1, 0).astype(np.float32)\n\n    df[\"price_clip\"] = df[\"price\"].clip(lower=PRICE_CLIP_LOW, upper=PRICE_CLIP_HIGH).astype(np.float32)\n    df[\"price_log\"]  = np.log1p(df[\"price_clip\"]).astype(np.float32)\n\n    df[\"is_loc_match\"] = (df[\"query_loc\"].astype(\"float32\") == df[\"item_loc\"].astype(\"float32\")).astype(np.int8)\n    df[\"is_cat_match\"] = (df[\"query_cat\"].astype(\"float32\") == df[\"item_cat_id\"].astype(\"float32\")).astype(np.int8)\n\n    textA = (df[\"query_text\"].astype(str) + \" \" + df[\"item_title\"].astype(str)).values\n    textB = (df[\"query_text\"].astype(str) + \" \" + df[\"item_description\"].astype(str)).values\n\n    XA_counts = hv.transform(textA)\n    XB_counts = hv.transform(textB)\n\n    XA = tfidf_transform_counts(XA_counts, idf_A, sublinear_tf=True, l2_norm=True)\n    XB = tfidf_transform_counts(XB_counts, idf_B, sublinear_tf=True, l2_norm=True)\n\n    ZA = svd_A.transform(XA).astype(np.float32)\n    ZB = svd_B.transform(XB).astype(np.float32)\n\n    ZA_df = pd.DataFrame(ZA, columns=[f\"tfidfA_svd_{j}\" for j in range(ZA.shape[1])])\n    ZB_df = pd.DataFrame(ZB, columns=[f\"tfidfB_svd_{j}\" for j in range(ZB.shape[1])])\n\n    df = pd.concat([df.reset_index(drop=True), ZA_df, ZB_df], axis=1)\n\n    df = df.drop(columns=[\"query_text\",\"item_title\",\"item_description\",\"price\",\"item_query_click_conv\"])\n\n    out_path = f\"{OUT_DIR_TRAIN}/part_{part:03d}.parquet\"\n    df.to_parquet(out_path, index=False)\n\n    rows_total += len(df)\n    part += 1\n\n    del df, textA, textB, XA_counts, XB_counts, XA, XB, ZA, ZB\n    gc.collect()\n\n    if bi % 10 == 0:\n        print(f\"FEAT batch={bi}, saved_parts={part}, rows_total={rows_total}\")\n\nprint(\"DONE. parts:\", part, \"rows:\", rows_total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:32:18.071636Z","iopub.execute_input":"2026-01-27T04:32:18.071887Z","iopub.status.idle":"2026-01-27T04:58:03.607761Z","shell.execute_reply.started":"2026-01-27T04:32:18.071855Z","shell.execute_reply":"2026-01-27T04:58:03.606949Z"}},"outputs":[{"name":"stdout","text":"FEAT batch=0, saved_parts=1, rows_total=30000\nFEAT batch=10, saved_parts=11, rows_total=330000\nFEAT batch=20, saved_parts=21, rows_total=630000\nFEAT batch=30, saved_parts=31, rows_total=930000\nFEAT batch=40, saved_parts=41, rows_total=1228576\nFEAT batch=50, saved_parts=51, rows_total=1528576\nFEAT batch=60, saved_parts=61, rows_total=1828576\nFEAT batch=70, saved_parts=71, rows_total=2127152\nFEAT batch=80, saved_parts=81, rows_total=2427152\nFEAT batch=90, saved_parts=91, rows_total=2727152\nFEAT batch=100, saved_parts=101, rows_total=3027152\nFEAT batch=110, saved_parts=111, rows_total=3325728\nFEAT batch=120, saved_parts=121, rows_total=3625728\nFEAT batch=130, saved_parts=131, rows_total=3925728\nFEAT batch=140, saved_parts=141, rows_total=4224304\nFEAT batch=150, saved_parts=151, rows_total=4524304\nFEAT batch=160, saved_parts=161, rows_total=4824304\nFEAT batch=170, saved_parts=171, rows_total=5124304\nFEAT batch=180, saved_parts=181, rows_total=5422880\nFEAT batch=190, saved_parts=191, rows_total=5722880\nFEAT batch=200, saved_parts=201, rows_total=6022880\nFEAT batch=210, saved_parts=211, rows_total=6321456\nFEAT batch=220, saved_parts=221, rows_total=6621456\nFEAT batch=230, saved_parts=231, rows_total=6921456\nFEAT batch=240, saved_parts=241, rows_total=7221456\nFEAT batch=250, saved_parts=251, rows_total=7520032\nDONE. parts: 260 rows: 7781790\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Preprocessing TF-IDF (Test)","metadata":{}},{"cell_type":"code","source":"import os, gc, json\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport joblib\nimport scipy.sparse as sp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:58:03.608980Z","iopub.execute_input":"2026-01-27T04:58:03.609238Z","iopub.status.idle":"2026-01-27T04:58:03.613784Z","shell.execute_reply.started":"2026-01-27T04:58:03.609214Z","shell.execute_reply":"2026-01-27T04:58:03.612942Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"TEST_PATH = \"/kaggle/input/datasets-avito/test-dset-small.parquet\"\nOUT_DIR_TEST = \"test_featurized_parts\"\nos.makedirs(OUT_DIR_TEST, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:58:03.614895Z","iopub.execute_input":"2026-01-27T04:58:03.615214Z","iopub.status.idle":"2026-01-27T04:58:03.628289Z","shell.execute_reply.started":"2026-01-27T04:58:03.615182Z","shell.execute_reply":"2026-01-27T04:58:03.627601Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"hv = joblib.load(\"hashing_vectorizer.joblib\")\nidf_A = joblib.load(\"idf_A.npy\")              \nidf_B = joblib.load(\"idf_B.npy\")              \nsvd_A = joblib.load(\"svd_A_128.joblib\")\nsvd_B = joblib.load(\"svd_B_128.joblib\")\n\nwith open(\"global_price_clip.json\", \"r\") as f:\n    clip = json.load(f)\nPRICE_CLIP_LOW = float(clip[\"PRICE_CLIP_LOW\"])\nPRICE_CLIP_HIGH = float(clip[\"PRICE_CLIP_HIGH\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:58:03.630059Z","iopub.execute_input":"2026-01-27T04:58:03.630383Z","iopub.status.idle":"2026-01-27T04:58:06.420265Z","shell.execute_reply.started":"2026-01-27T04:58:03.630360Z","shell.execute_reply":"2026-01-27T04:58:06.419455Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def tfidf_transform_counts(X_counts_csr, idf_vec, sublinear_tf=True, l2_norm=True):\n    X = X_counts_csr.tocsr(copy=True)\n    if sublinear_tf:\n        X.data = np.log1p(X.data)\n    X = X.multiply(idf_vec)\n    if l2_norm:\n        row_sums = np.sqrt(X.power(2).sum(axis=1)).A1\n        row_sums[row_sums == 0] = 1.0\n        X = X.multiply(1.0 / row_sums[:, None])\n    return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:58:06.421231Z","iopub.execute_input":"2026-01-27T04:58:06.421582Z","iopub.status.idle":"2026-01-27T04:58:06.426825Z","shell.execute_reply.started":"2026-01-27T04:58:06.421547Z","shell.execute_reply":"2026-01-27T04:58:06.426073Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"BATCH_SIZE = 30_000  \n\ndataset = ds.dataset(TEST_PATH, format=\"parquet\")\nscanner = dataset.scanner(columns=[\n    \"query_id\",\"item_id\",\n    \"query_text\",\"item_title\",\"item_description\",\n    \"query_cat\",\"query_mcat\",\"query_loc\",\n    \"item_cat_id\",\"item_mcat_id\",\"item_loc\",\n    \"price\",\"item_query_click_conv\",\n], batch_size=BATCH_SIZE)\n\npart = 0\nrows_total = 0\n\nfor bi, batch in enumerate(scanner.to_batches()):\n    df = pa.Table.from_batches([batch]).to_pandas()\n\n    df[\"item_title\"] = df[\"item_title\"].fillna(\"\")\n    df[\"item_description\"] = df[\"item_description\"].fillna(\"\")\n    df[\"query_mcat\"] = df[\"query_mcat\"].fillna(-1)\n\n    df[\"conv_missing\"] = (df[\"item_query_click_conv\"] == -1).astype(np.int8)\n    df[\"conv_val\"] = df[\"item_query_click_conv\"].where(df[\"item_query_click_conv\"] != -1, 0).astype(np.float32)\n\n    df[\"price_clip\"] = df[\"price\"].clip(lower=PRICE_CLIP_LOW, upper=PRICE_CLIP_HIGH).astype(np.float32)\n    df[\"price_log\"]  = np.log1p(df[\"price_clip\"]).astype(np.float32)\n\n    df[\"is_loc_match\"] = (df[\"query_loc\"].astype(\"float32\") == df[\"item_loc\"].astype(\"float32\")).astype(np.int8)\n    df[\"is_cat_match\"] = (df[\"query_cat\"].astype(\"float32\") == df[\"item_cat_id\"].astype(\"float32\")).astype(np.int8)\n\n    textA = (df[\"query_text\"].astype(str) + \" \" + df[\"item_title\"].astype(str)).values\n    textB = (df[\"query_text\"].astype(str) + \" \" + df[\"item_description\"].astype(str)).values\n\n    XA_counts = hv.transform(textA)\n    XB_counts = hv.transform(textB)\n\n    XA = tfidf_transform_counts(XA_counts, idf_A, sublinear_tf=True, l2_norm=True)\n    XB = tfidf_transform_counts(XB_counts, idf_B, sublinear_tf=True, l2_norm=True)\n\n    ZA = svd_A.transform(XA).astype(np.float32)\n    ZB = svd_B.transform(XB).astype(np.float32)\n\n    ZA_df = pd.DataFrame(ZA, columns=[f\"tfidfA_svd_{j}\" for j in range(ZA.shape[1])])\n    ZB_df = pd.DataFrame(ZB, columns=[f\"tfidfB_svd_{j}\" for j in range(ZB.shape[1])])\n    df = pd.concat([df.reset_index(drop=True), ZA_df, ZB_df], axis=1)\n\n    df = df.drop(columns=[\"query_text\",\"item_title\",\"item_description\",\"price\",\"item_query_click_conv\"])\n\n    out_path = f\"{OUT_DIR_TEST}/part_{part:03d}.parquet\"\n    df.to_parquet(out_path, index=False)\n\n    rows_total += len(df)\n    part += 1\n\n    del df, ZA, ZB, ZA_df, ZB_df, XA_counts, XB_counts, XA, XB, textA, textB\n    gc.collect()\n\n    if bi % 10 == 0:\n        print(f\"TEST batch={bi}, saved_parts={part}, rows_total={rows_total}\")\n\nprint(\"DONE. test parts:\", part, \"rows:\", rows_total)\nprint(\"Output dir:\", OUT_DIR_TEST)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T04:58:06.427691Z","iopub.execute_input":"2026-01-27T04:58:06.427950Z","iopub.status.idle":"2026-01-27T04:59:19.575543Z","shell.execute_reply.started":"2026-01-27T04:58:06.427917Z","shell.execute_reply":"2026-01-27T04:59:19.574813Z"}},"outputs":[{"name":"stdout","text":"TEST batch=0, saved_parts=1, rows_total=30000\nTEST batch=10, saved_parts=11, rows_total=330000\nDONE. test parts: 12 rows: 335348\nOutput dir: test_featurized_parts\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import glob, gc\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostRanker, Pool","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PARTS_DIR = \"train_featurized_parts\"\nparts = sorted(glob.glob(f\"{PARTS_DIR}/part_*.parquet\"))\nprint(\"n_parts:\", len(parts))\n\nval_qids = set(pd.read_parquet(\"qids_fold_0.parquet\")[\"query_id\"].values)\n\ntrain_qids = set(pd.read_parquet(\"qids_fold_1.parquet\")[\"query_id\"].values) | \\\n             set(pd.read_parquet(\"qids_fold_2.parquet\")[\"query_id\"].values)\n\nprint(\"val_qids:\", len(val_qids), \"train_qids:\", len(train_qids))\n\ntrain_chunks, val_chunks = [], []\n\nfor i, p in enumerate(parts):\n    df = pd.read_parquet(p)\n\n    m_val = df[\"query_id\"].isin(val_qids)\n    if m_val.any():\n        val_chunks.append(df[m_val])\n\n    m_tr = df[\"query_id\"].isin(train_qids)\n    if m_tr.any():\n        train_chunks.append(df[m_tr])\n\n    if i % 20 == 0:\n        tr_rows = sum(len(x) for x in train_chunks)\n        va_rows = sum(len(x) for x in val_chunks)\n        print(f\"processed {i}/{len(parts)} | tr_rows={tr_rows} va_rows={va_rows}\")\n\n    del df\n    gc.collect()\n\ntr = pd.concat(train_chunks, ignore_index=True)\nva = pd.concat(val_chunks, ignore_index=True)\ndel train_chunks, val_chunks\ngc.collect()\n\nprint(\"FINAL tr shape:\", tr.shape, \"va shape:\", va.shape)\nprint(\"tr target mean:\", tr[\"item_contact\"].mean(), \"va target mean:\", va[\"item_contact\"].mean())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET = \"item_contact\"\nGROUP = \"query_id\"\ndrop_cols = [TARGET, \"item_id\", \"query_id\"]\nfeatures = [c for c in tr.columns if c not in drop_cols]\n\nprint(\"n_features:\", len(features))\n\ntr = tr.sort_values(GROUP, kind=\"mergesort\").reset_index(drop=True)  # mergesort stable, часто чуть аккуратнее по памяти\nva = va.sort_values(GROUP, kind=\"mergesort\").reset_index(drop=True)\n\ncat_features = []\n\ntrain_pool = Pool(\n    tr[features],\n    label=tr[TARGET].astype(int),\n    group_id=tr[GROUP].values,\n    cat_features=cat_features\n)\nvalid_pool = Pool(\n    va[features],\n    label=va[TARGET].astype(int),\n    group_id=va[GROUP].values,\n    cat_features=cat_features\n)\n\nranker = CatBoostRanker(\n    loss_function=\"YetiRank\",\n    eval_metric=\"NDCG:top=10\",\n    iterations=3000,\n    learning_rate=0.05,\n    depth=8,\n    random_seed=42,\n    verbose=100,\n    task_type=\"GPU\"\n)\n\nranker.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n\nprint(\"best_iteration:\", ranker.get_best_iteration())\nprint(\"best_score:\", ranker.get_best_score())\n\nranker.save_model(\"catboost_ranker_40pct.cbm\")\nprint(\"Saved model: catboost_ranker_40pct.cbm\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict + Submission","metadata":{}},{"cell_type":"code","source":"import glob, gc\nimport pandas as pd\nfrom catboost import CatBoostRanker","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ranker = CatBoostRanker()\nranker.load_model(\"catboost_ranker_40pct.cbm\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PARTS_DIR_TEST = \"test_featurized_parts\"\ntest_parts = sorted(glob.glob(f\"{PARTS_DIR_TEST}/part_*.parquet\"))\nprint(\"n_test_parts:\", len(test_parts))\n\npred_parts = []\ntotal = 0\n\nfor i, p in enumerate(test_parts):\n    df = pd.read_parquet(p)\n\n    feature_cols = [c for c in df.columns if c not in [\"query_id\", \"item_id\"]]\n    df[\"score\"] = ranker.predict(df[feature_cols])\n\n    pred_parts.append(df[[\"query_id\", \"item_id\", \"score\"]])\n    total += len(df)\n\n    del df\n    gc.collect()\n\n    print(f\"pred {i+1}/{len(test_parts)} total_rows={total}\")\n\npred = pd.concat(pred_parts, ignore_index=True)\ndel pred_parts\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pred.sort_values([\"query_id\", \"score\"], ascending=[True, False])[[\"query_id\", \"item_id\"]]\nsubmission_df.to_csv(\"solution.csv\", header=[\"query_id\", \"item_id\"], index=False)\n\nprint(\"Saved solution.csv shape:\", submission_df.shape)\nprint(submission_df.head(20))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}